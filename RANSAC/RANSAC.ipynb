{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhATaaotWpYY"
   },
   "source": [
    "# RANSAC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaFe_Ot-ziRV"
   },
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "error",
     "timestamp": 1612557788001,
     "user": {
      "displayName": "Taeyoung Lee",
      "photoUrl": "",
      "userId": "07906618747313337531"
     },
     "user_tz": 300
    },
    "id": "XOGSROrK6oGt",
    "outputId": "201bf5ab-4d81-436e-cb82-4c62e1feaf32"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy.signal\n",
    "import scipy.linalg\n",
    "import mae6292.tools as mae6292\n",
    "import importlib\n",
    "\n",
    "from mae6292.imshow import cv2_imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEUEyf2dPM-4"
   },
   "source": [
    "## RANSAC Localization with Tsai's Method\n",
    "\n",
    "We apply RANSAC to localization for the KITTI dataset.\n",
    "\n",
    "### Load Data\n",
    "The following data are given:\n",
    "\n",
    "1.  Database image `data/000000.png`\n",
    "2.  Keypoints in the **database** image `data/keypoints.txt`\n",
    "3.  3D location of the keypoints in the W-frame `data/p_W_landmarks.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_database = cv2.imread('data/000000.png',cv2.IMREAD_GRAYSCALE)\n",
    "keypoints_database = list(map(tuple, np.loadtxt('data/keypoints.txt', dtype='int')-1)) # list of (row, col)\n",
    "K = np.loadtxt('data/K.txt')\n",
    "p_W_landmarks = np.loadtxt('data/p_W_landmarks.txt').T\n",
    "\n",
    "plt.figure(dpi=120)\n",
    "plt.imshow(img_database,cmap='gray')\n",
    "plt.plot(np.array(keypoints_database).T[1,:],np.array(keypoints_database).T[0,:],'r+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Keypoints and World Coordinates\n",
    "\n",
    "Suppose a query image is given by `data/000001.png`. We wish to find the relative pose of the **query** image.\n",
    "\n",
    "Since the 3D world coordinates of the keypoints are already given above, we can utilize Tsai's method. (See the course note \"Camera Calibration\", HW#5, `python_example/Harris_feature`)\n",
    "\n",
    "For Tsai's method, we need key points in the query image, and the corresponding 3D location, which can be discovered by matching the features of the query image to the database image. In other words, \n",
    "\n",
    "(keypoints in the query image `000001`) ==(Harris detector/matching)==> (keypoints in the database image `000000`) ==(given data)==> (3D world coordinates)\n",
    "\n",
    "We need RANSAC as the matched features may contain outliers.\n",
    "\n",
    "\n",
    "For the keypoint matching, we will utilize the Harris detector that we developed in HW#5. Recall the following functions:\n",
    "\n",
    "```\n",
    "mae6292.harris_corner(img, W, kappa):\n",
    "    # Compute harris corner score\n",
    "    # INPUT\n",
    "    #   img: gray scale image\n",
    "    #   W: harris patch size (patch width= 2*W+1)\n",
    "    #   kappa: kappa in the harris score\n",
    "    # OUTPUT\n",
    "    #   harris_score: array of the same size as the input image\n",
    "\n",
    "mae6292.select_keypoints(score, N_keypoint, W_nms):\n",
    "    # Select keypoints from Harris score\n",
    "    # INPUT\n",
    "    #   score: array of Harris score\n",
    "    #   N_keypoints: number of keypoints to be detected\n",
    "    #   W_nms: patch size for non maximum supression\n",
    "    # OUTPUT\n",
    "    #   keypoints: list of tuples (row,col) for the keypoints\n",
    "\n",
    "mae6292.describe_keypoints(img, keypoints, W):\n",
    "    # Extract descriptors as patches centered at keypoints \n",
    "    # INPUT\n",
    "    #   img: image\n",
    "    #   keypoints: list of tuples (row,col) for keypoints\n",
    "    #   W: descriptor patch size\n",
    "    # OUTPUT\n",
    "    # descriptor: array of len(keypoints) by (2*W+1)**2 \n",
    "\n",
    "mae6292.match_descriptors(descriptors_new, descriptors_old, lambda_match):\n",
    "    # Match descriptors\n",
    "    # INPUT\n",
    "    # descriptors_new (query): array of N_descriptor_new by (2*W+1)**2 \n",
    "    # descriptors_old (database): array of N_descriptor_old by (2*W+1)**2 \n",
    "    # lambda_match: match if distance >= lambda_match * min_nonzero_distance\n",
    "    # OUTPUT\n",
    "    # unique_match: \n",
    "    # if unique_match >= 0:\n",
    "    #   descriptors_new[:,i] is closest to descriptors_old[:,match[i]]\n",
    "    # else\n",
    "    #   descriptors_new[:,i] is not matched to any\n",
    "    # distance: distance \n",
    "\n",
    "```\n",
    "\n",
    "In short, \n",
    "\n",
    "1.  Find the keypoints in the query image `000001`, and compute descriptors.\n",
    "2.  Compute descriptors in the database image `000000`. (No need to compute keypoints, as they are already given).\n",
    "3.  Match descriptors between the query image and thh database image.\n",
    "4.  Matched keypoints in the query, and the corresponding 3D locations are saved for DLT.\n",
    "\n",
    "We choose parameters from HW#5 as\n",
    "```\n",
    "W_harris_patch = 4\n",
    "harris_kappa = 0.08\n",
    "N_nms = 8\n",
    "W_descripto = 9\n",
    "match_lambda = 5\n",
    "N_keypoints = 1000\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_harris_patch = 4\n",
    "harris_kappa = 0.08\n",
    "W_nms = 8\n",
    "W_descriptor = 9\n",
    "lambda_match = 5\n",
    "N_keypoint = 1000\n",
    "\n",
    "# 1. imq_query: detect keypoints and compute descriptor\n",
    "img_query = cv2.imread('data/000001.png',cv2.IMREAD_GRAYSCALE)\n",
    "harris_score_query = mae6292.harris_corner(img_query, W_harris_patch, harris_kappa)\n",
    "keypoints_query = mae6292.select_keypoints(harris_score_query, N_keypoint, W_nms)\n",
    "descriptors_query = mae6292.describe_keypoints(img_query, keypoints_query, W_descriptor)\n",
    "\n",
    "# 2. imq_database: compute descriptor\n",
    "descriptors_database = mae6292.describe_keypoints(img_database, keypoints_database, W_descriptor)\n",
    "\n",
    "# 3. match descriptors \n",
    "match, distance = mae6292.match_descriptors(descriptors_query, descriptors_database, lambda_match)\n",
    "index_match = np.where(match>=0)[0].tolist()\n",
    "\n",
    "# 4. extract matched keypoints and 3D world coordinates\n",
    "keypoints_query_matched = [keypoints_query[i] for i in index_match]\n",
    "keypoints_database_matched = [keypoints_database[match[i]] for i in index_match]\n",
    "p_W_matched = p_W_landmarks[:, match[index_match]]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(dpi=120)\n",
    "plt.imshow(img_database,cmap='gray')\n",
    "\n",
    "for i in index_match: # iteration on matched keypoints\n",
    "    v_q, u_q = keypoints_query[i] # matched keypoints on img1\n",
    "    v_d, u_d = keypoints_database[match[i]] # matched keypoints on img\n",
    "    \n",
    "    plt.plot(u_q, v_q, 'r+') \n",
    "    plt.plot(u_d, v_d, 'b+')\n",
    "    plt.plot([u_d,u_q],[v_d,v_q],'r')\n",
    "\n",
    "print('total number of keypoints = ', len(keypoints_query) )\n",
    "print('number of matched discriptors = ', len(index_match) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANSAC Localization\n",
    "\n",
    "Now, in the query image, we have \n",
    "\n",
    "1. pixel coordinates of features, `keypoints_query_matched`\n",
    "2. 3D world coordinates of features, `p_W_matched`\n",
    "\n",
    "However, as visualized above, there are a lot of incorrect matches, or outliers.\n",
    "\n",
    "\n",
    "We will perform robust DLT (Tsai's method) while eliminating outliers with RANSAC \n",
    "\n",
    "0. Set `N_inliers_max = 0`\n",
    "1.  Randomly select $s=6$ points using [np.random.choice(..., ..., replace=False)](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) It is important that no points are repeated!\n",
    "2.  Estimate the pose using `mae6292.estimate_pose_DLT(p,P,K)`, which is a special case of Tsai's method when $K$ is known.\n",
    "\n",
    "\n",
    "```\n",
    "    R, T, R_T = mae6292.estimate_pose_DLT(p, P, K):\n",
    "        # DLT using Tsai's method assuming that K is known\n",
    "        # INPUT\n",
    "        # p : homogeneous coordinates of pixel in the image frame\n",
    "        # P : homogeneous coordinates of points in the world frame\n",
    "        # OUTPUT\n",
    "        # R, T, [R|T]\n",
    "```\n",
    "\n",
    "NOTE: keypoints are saved as the list of `(row, col)`. Therefore, when converting `keypoints` to `p=[u,v,1]`, the order should be flipped!\n",
    "\n",
    "3.  Using the estimated $(R,T)$, project each of `p_W_matched` into the image plane using the perspective projection `K@[R|T]`, and compute the reprojection error. \n",
    "If the reprojection error is less than `tol_inlier=10` pixels, then declare it as inlier.\n",
    "\n",
    "4.  If `N_inliers > N_inliers_max`, save the current inliers.\n",
    "5.  Repeat the above 1-4.\n",
    "6.  Estimate the pose using the inliers only.\n",
    "\n",
    "Since $s=6$, we need 292 iterations when $w=0.5$ and $p=0.99$. One can estimate the inlier ratio online, and change the number of iterations accordingly for early stopping.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_inliers_max = 0\n",
    "N_iter = 292\n",
    "tol_inlier = 10\n",
    "display_iter = True\n",
    "\n",
    "# convert list of keypoint to array\n",
    "p_matched = np.array(keypoints_query_matched).T \n",
    "# flip (row,col) into image coordinate (u,v)\n",
    "p_matched = p_matched[[1,0],:] \n",
    "# transform to homogeneous coordinates\n",
    "p_matched = np.concatenate((p_matched, np.ones((1,p_matched.shape[1])) ), axis=0) \n",
    "\n",
    "# extract matched 3D points and transform to homogeneous coordinates\n",
    "p_W_matched = p_W_landmarks[:, match[index_match]]\n",
    "p_W_matched = np.concatenate((p_W_matched, np.ones((1,p_W_matched.shape[1])) ), axis=0) \n",
    "\n",
    "for i_iter in range(N_iter):\n",
    "\n",
    "    # sample 6 points from the matched keypoints and estimate pose using them\n",
    "    i_sample = np.random.choice(len(keypoints_query_matched), 6, replace=False)\n",
    "    R, T, R_T = mae6292.estimate_pose_DLT(p_matched[:, i_sample], p_W_matched[:,i_sample], K)\n",
    "\n",
    "    # compute the reprojection error\n",
    "    p_matched_est = K @ R_T @ p_W_matched\n",
    "    error = np.sqrt(np.sum((p_matched_est[0:2,:]/p_matched_est[2,:]-p_matched[0:2,:]/p_matched[2,:])**2, axis=0))\n",
    "    \n",
    "    # identify inliers\n",
    "    i_inliers = np.where(error < tol_inlier)[0]\n",
    "\n",
    "    # if the number of inliers is the best, save the list of inliers\n",
    "    if len(i_inliers) > N_inliers_max:\n",
    "        i_inliers_best = i_inliers\n",
    "        N_inliers_max = len(i_inliers)\n",
    "        output = 'i_iter=%d, N_inliers=%d, w=%.2f' % (i_iter,N_inliers_max,N_inliers_max/p_matched.shape[1])\n",
    "        print(output)\n",
    "\n",
    "        if display_iter and i_iter !=1:\n",
    "            # visualize inliers\n",
    "            plt.figure(dpi=120)\n",
    "            plt.imshow(img_query,cmap='gray')\n",
    "            plt.title(output)\n",
    "            for i in i_inliers:\n",
    "                v_q, u_q = keypoints_query_matched[i] # matched keypoints on img1\n",
    "                v_d, u_d = keypoints_database_matched[i] # matched keypoints on img\n",
    "\n",
    "                plt.plot(u_q, v_q, 'r+') \n",
    "                plt.plot(u_d, v_d, 'b+')\n",
    "                plt.plot([u_d,u_q],[v_d,v_q],'r')\n",
    "\n",
    "# estimate the pose using inliers only\n",
    "R, T, M = mae6292.estimate_pose_DLT(p_matched[:, i_inliers_best], p_W_matched[:,i_inliers_best], K)\n",
    "print('R=',R)\n",
    "print('T=',T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "YaFe_Ot-ziRV"
   ],
   "name": "image_filtering.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
